#!/usr/bin/env python3
"""
ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®ï¼ˆGram-.fastaï¼‰è®­ç»ƒHydrAMPæ¨¡å‹çš„å®Œæ•´æµç¨‹
æ³¨æ„ï¼šè¿™ä¸ªè„šæœ¬éœ€è¦å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†æ‰èƒ½æ­£å¸¸è¿è¡Œ
"""

import os
import sys
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

# è®¾ç½®GPUé…ç½®
config = tf.compat.v1.ConfigProto(
    gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8),
)
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(session)

# å¯¼å…¥HydrAMPæ¨¡å—
import amp.data_utils.data_loader as data_loader
import amp.data_utils.sequence as du_sequence
from amp.config import hydra, MIN_LENGTH, MAX_LENGTH, LATENT_DIM, MIN_KL, RCL_WEIGHT, HIDDEN_DIM, MAX_TEMPERATURE
from amp.models.decoders import amp_expanded_decoder
from amp.models.encoders import amp_expanded_encoder
from amp.models.master import master
from amp.utils import basic_model_serializer, callback, generator
from keras import backend, layers
from keras.optimizers import Adam

def prepare_custom_data(fasta_file, output_positive_csv):
    """
    å‡†å¤‡è‡ªå®šä¹‰æ•°æ®ï¼šå°†FASTAè½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„CSVæ ¼å¼
    """
    print(f"æ­£åœ¨å¤„ç†FASTAæ–‡ä»¶: {fasta_file}")
    
    # å®šä¹‰æ ‡å‡†æ°¨åŸºé…¸
    STANDARD_AA = set('ACDEFGHIKLMNPQRSTVWY')
    
    def check_if_std_aa(seq):
        return all(aa in STANDARD_AA for aa in seq.upper())
    
    # è§£æFASTAæ–‡ä»¶
    sequences = []
    ids = []
    current_id = ''
    current_seq = ''
    
    with open(fasta_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('>'):
                if current_seq:
                    sequences.append(current_seq)
                    ids.append(current_id)
                current_id = line[1:]
                current_seq = ''
            else:
                current_seq += line
        
        # æ·»åŠ æœ€åä¸€ä¸ªåºåˆ—
        if current_seq:
            sequences.append(current_seq)
            ids.append(current_id)
    
    # è¿‡æ»¤åºåˆ—ï¼šé•¿åº¦<=25ä¸”åªåŒ…å«æ ‡å‡†æ°¨åŸºé…¸
    filtered_data = []
    for seq_id, seq in zip(ids, sequences):
        if MIN_LENGTH <= len(seq) <= MAX_LENGTH and check_if_std_aa(seq):
            filtered_data.append({'Name': seq_id, 'Sequence': seq})
    
    # ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(filtered_data)
    df.to_csv(output_positive_csv, index=False)
    
    print(f"å·²ä¿å­˜{len(filtered_data)}æ¡æœ‰æ•ˆåºåˆ—åˆ°: {output_positive_csv}")
    return len(filtered_data)

def create_minimal_training_setup():
    """
    åˆ›å»ºæœ€å°åŒ–è®­ç»ƒé…ç½®
    è­¦å‘Šï¼šè¿™ä¸ªé…ç½®å¯èƒ½ä¸ä¼šäº§ç”Ÿå¥½çš„ç»“æœï¼Œå› ä¸ºç¼ºå°‘å¿…è¦çš„è®­ç»ƒæ•°æ®
    """
    print("âš ï¸  è­¦å‘Šï¼šå½“å‰é…ç½®ç¼ºå°‘ä»¥ä¸‹å¿…è¦æ•°æ®ï¼š")
    print("   - è´Ÿæ ·æœ¬æ•°æ®ï¼ˆéAMPåºåˆ—ï¼‰")
    print("   - MICæ•°æ®ï¼ˆæœ€å°æŠ‘èŒæµ“åº¦ï¼‰")
    print("   - UniProtèƒŒæ™¯æ•°æ®")
    print("   - é¢„è®­ç»ƒçš„AMPå’ŒMICåˆ†ç±»å™¨")
    print("   è¿™å°†ä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ï¼")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        'data/unlabelled_negative.csv',  # è´Ÿæ ·æœ¬
        'data/mic_data.csv',             # MICæ•°æ®
        'data/Uniprot_0_25_train.csv',   # UniProtè®­ç»ƒæ•°æ®
        'data/Uniprot_0_25_val.csv',     # UniProtéªŒè¯æ•°æ®
        'models/amp_classifier',         # AMPåˆ†ç±»å™¨
        'models/mic_classifier'          # MICåˆ†ç±»å™¨
    ]
    
    missing_files = []
    for file_path in required_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print(f"\nâŒ ç¼ºå°‘ä»¥ä¸‹å¿…è¦æ–‡ä»¶ï¼š")
        for file_path in missing_files:
            print(f"   - {file_path}")
        print("\nè¯·å…ˆè¿è¡Œ 'sh get_data.sh' ä¸‹è½½å®Œæ•´æ•°æ®é›†")
        return False
    
    return True

def train_hydramp_with_custom_data(custom_positive_csv):
    """
    ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒHydrAMPæ¨¡å‹
    """
    print("å¼€å§‹è®­ç»ƒHydrAMPæ¨¡å‹...")
    
    # è®¾ç½®éšæœºç§å­
    seed = 7
    np.random.seed(seed)
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    kl_weight = backend.variable(MIN_KL, name="kl_weight")
    tau = backend.variable(MAX_TEMPERATURE, name="temperature")
    
    # 1. åŠ è½½æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    
    # è‡ªå®šä¹‰æ­£æ ·æœ¬æ•°æ®
    custom_df = pd.read_csv(custom_positive_csv)
    custom_sequences = custom_df['Sequence'].tolist()
    custom_x = du_sequence.pad(du_sequence.to_one_hot(custom_sequences))
    custom_y = np.ones(len(custom_sequences))  # å…¨éƒ¨æ ‡è®°ä¸ºæ­£æ ·æœ¬
    
    # åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        data_manager = data_loader.AMPDataManager(
            'data/unlabelled_positive.csv',
            'data/unlabelled_negative.csv',
            min_len=MIN_LENGTH,
            max_len=MAX_LENGTH
        )
        amp_x, amp_y = data_manager.get_merged_data()
        
        # åˆå¹¶æ•°æ®
        combined_x = np.vstack([amp_x, custom_x])
        combined_y = np.concatenate([amp_y, custom_y])
        
    except Exception as e:
        print(f"æ— æ³•åŠ è½½åŸå§‹æ•°æ®ï¼Œä»…ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®: {e}")
        combined_x = custom_x
        combined_y = custom_y
    
    # æ•°æ®åˆ†å‰²
    amp_x_train, amp_x_test, amp_y_train, amp_y_test = train_test_split(
        combined_x, combined_y, test_size=0.1, random_state=36
    )
    amp_x_train, amp_x_val, amp_y_train, amp_y_val = train_test_split(
        amp_x_train, amp_y_train, test_size=0.2, random_state=36
    )
    
    # 2. åŠ è½½MICæ•°æ®
    try:
        ecoli_df = pd.read_csv('data/mic_data.csv')
        mask = (ecoli_df['sequence'].str.len() <= MAX_LENGTH) & (ecoli_df['sequence'].str.len() >= MIN_LENGTH)
        ecoli_df = ecoli_df.loc[mask]
        mic_x = du_sequence.pad(du_sequence.to_one_hot(ecoli_df['sequence']))
        mic_y = ecoli_df.value.values
        
        mic_x_train, mic_x_test, mic_y_train, mic_y_test = train_test_split(
            mic_x, mic_y, test_size=0.1, random_state=36
        )
        mic_x_train, mic_x_val, mic_y_train, mic_y_val = train_test_split(
            mic_x_train, mic_y_train, test_size=0.2, random_state=36
        )
        
    except Exception as e:
        print(f"æ— æ³•åŠ è½½MICæ•°æ®: {e}")
        # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®çš„ä¸€éƒ¨åˆ†ä½œä¸ºMICæ•°æ®
        mic_x_train, mic_x_val = amp_x_train[:100], amp_x_val[:50]
        mic_y_train, mic_y_val = np.random.uniform(0, 2, 100), np.random.uniform(0, 2, 50)
    
    # 3. åŠ è½½UniProtæ•°æ®
    try:
        uniprot_x_train = np.array(du_sequence.pad(du_sequence.to_one_hot(
            pd.read_csv('data/Uniprot_0_25_train.csv').Sequence
        )))
        uniprot_x_val = np.array(du_sequence.pad(du_sequence.to_one_hot(
            pd.read_csv('data/Uniprot_0_25_val.csv').Sequence
        )))
    except Exception as e:
        print(f"æ— æ³•åŠ è½½UniProtæ•°æ®: {e}")
        # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®çš„ä¸€éƒ¨åˆ†
        uniprot_x_train, uniprot_x_val = amp_x_train[:1000], amp_x_val[:200]
    
    # 4. åŠ è½½é¢„è®­ç»ƒåˆ†ç±»å™¨
    try:
        bms = basic_model_serializer.BasicModelSerializer()
        amp_classifier = bms.load_model('models/amp_classifier')
        mic_classifier = bms.load_model('models/mic_classifier')
        amp_classifier_model = amp_classifier()
        mic_classifier_model = mic_classifier()
        print("æˆåŠŸåŠ è½½é¢„è®­ç»ƒåˆ†ç±»å™¨")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½é¢„è®­ç»ƒåˆ†ç±»å™¨: {e}")
        print("è®­ç»ƒæ— æ³•ç»§ç»­ï¼Œè¯·ç¡®ä¿åˆ†ç±»å™¨æ–‡ä»¶å­˜åœ¨")
        return False
    
    # 5. æ„å»ºæ¨¡å‹
    print("æ­£åœ¨æ„å»ºæ¨¡å‹...")
    
    encoder = amp_expanded_encoder.AMPEncoderFactory.get_default(HIDDEN_DIM, LATENT_DIM, MAX_LENGTH)
    decoder = amp_expanded_decoder.AMPDecoderFactory.build_default(LATENT_DIM + 2, tau, MAX_LENGTH)
    
    master_model = master.MasterAMPTrainer(
        amp_classifier=amp_classifier,
        mic_classifier=mic_classifier,
        encoder=encoder,
        decoder=decoder,
        kl_weight=kl_weight,
        rcl_weight=RCL_WEIGHT,
        master_optimizer=Adam(lr=1e-3),
        loss_weights=hydra,
    )
    
    # æ„å»ºKerasæ¨¡å‹
    input_to_encoder = layers.Input(shape=(MAX_LENGTH,))
    encoder_model = encoder(input_to_encoder)
    input_to_decoder = layers.Input(shape=(LATENT_DIM + 2,))
    decoder_model = decoder(input_to_decoder)
    
    master_keras_model = master_model.build(input_shape=(MAX_LENGTH, 21))
    
    # 6. å‡†å¤‡è®­ç»ƒæ•°æ®
    print("æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # é¢„æµ‹åˆ†ç±»å™¨è¾“å‡º
    amp_amp_train = amp_classifier_model.predict(amp_x_train, verbose=1, batch_size=10000).reshape(len(amp_x_train))
    amp_mic_train = mic_classifier_model.predict(amp_x_train, verbose=1, batch_size=10000).reshape(len(amp_x_train))
    amp_amp_val = amp_classifier_model.predict(amp_x_val, verbose=1, batch_size=10000).reshape(len(amp_x_val))
    amp_mic_val = mic_classifier_model.predict(amp_x_val, verbose=1, batch_size=10000).reshape(len(amp_x_val))
    
    mic_amp_train = amp_classifier_model.predict(mic_x_train, verbose=1, batch_size=10000).reshape(len(mic_x_train))
    mic_mic_train = mic_classifier_model.predict(mic_x_train, verbose=1, batch_size=10000).reshape(len(mic_x_train))
    mic_amp_val = amp_classifier_model.predict(mic_x_val, verbose=1, batch_size=10000).reshape(len(mic_x_val))
    mic_mic_val = mic_classifier_model.predict(mic_x_val, verbose=1, batch_size=10000).reshape(len(mic_x_val))
    
    uniprot_amp_train = amp_classifier_model.predict(uniprot_x_train, verbose=1, batch_size=10000).reshape(len(uniprot_x_train))
    uniprot_mic_train = mic_classifier_model.predict(uniprot_x_train, verbose=1, batch_size=10000).reshape(len(uniprot_x_train))
    uniprot_amp_val = amp_classifier_model.predict(uniprot_x_val, verbose=1, batch_size=10000).reshape(len(uniprot_x_val))
    uniprot_mic_val = mic_classifier_model.predict(uniprot_x_val, verbose=1, batch_size=10000).reshape(len(uniprot_x_val))
    
    # 7. è®¾ç½®å›è°ƒå‡½æ•°
    vae_callback = callback.VAECallback(
        encoder=encoder_model,
        decoder=decoder_model,
        tau=tau,
        kl_weight=kl_weight,
        amp_classifier=amp_classifier_model,
        mic_classifier=mic_classifier_model,
    )
    
    sm_callback = callback.SaveModelCallback(
        model=master_model,
        model_save_path="models/custom_trained/",
        name="HydrAMP_Custom"
    )
    
    # 8. åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
    training_generator = generator.concatenated_generator(
        uniprot_x_train, uniprot_amp_train, uniprot_mic_train,
        amp_x_train, amp_amp_train, amp_mic_train,
        mic_x_train, mic_amp_train, mic_mic_train,
        128
    )
    
    validation_generator = generator.concatenated_generator(
        uniprot_x_val, uniprot_amp_val, uniprot_mic_val,
        amp_x_val, amp_amp_val, amp_mic_val,
        mic_x_val, mic_amp_val, mic_mic_val,
        128
    )
    
    # 9. å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    
    # è®¡ç®—steps
    total_train_samples = len(uniprot_x_train) + len(amp_x_train) + len(mic_x_train)
    total_val_samples = len(uniprot_x_val) + len(amp_x_val) + len(mic_x_val)
    
    steps_per_epoch = max(1, total_train_samples // 128)
    validation_steps = max(1, total_val_samples // 128)
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {total_train_samples}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {total_val_samples}")
    print(f"Steps per epoch: {steps_per_epoch}")
    print(f"Validation steps: {validation_steps}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("models/custom_trained", exist_ok=True)
    
    try:
        history = master_keras_model.fit_generator(
            training_generator,
            steps_per_epoch=steps_per_epoch,
            epochs=30,  # å‡å°‘epochæ•°é‡è¿›è¡Œæµ‹è¯•
            validation_data=validation_generator,
            validation_steps=validation_steps,
            callbacks=[vae_callback, sm_callback],
        )
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: models/custom_trained/")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """
    ä¸»å‡½æ•°ï¼šå¤„ç†è‡ªå®šä¹‰æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹
    """
    print("=== HydrAMP è‡ªå®šä¹‰æ•°æ®è®­ç»ƒè„šæœ¬ ===")
    
    # 1. å‡†å¤‡è‡ªå®šä¹‰æ•°æ®
    fasta_file = "Gram-.fasta"
    custom_positive_csv = "custom_positive.csv"
    
    if not os.path.exists(fasta_file):
        print(f"âŒ æ‰¾ä¸åˆ°FASTAæ–‡ä»¶: {fasta_file}")
        return
    
    # è½¬æ¢FASTAä¸ºCSV
    num_sequences = prepare_custom_data(fasta_file, custom_positive_csv)
    
    if num_sequences < 10:
        print("âŒ æœ‰æ•ˆåºåˆ—æ•°é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
        return
    
    # 2. æ£€æŸ¥è®­ç»ƒç¯å¢ƒ
    if not create_minimal_training_setup():
        print("âŒ è®­ç»ƒç¯å¢ƒä¸å®Œæ•´")
        return
    
    # 3. å¼€å§‹è®­ç»ƒ
    success = train_hydramp_with_custom_data(custom_positive_csv)
    
    if success:
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("ç°åœ¨å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œç”Ÿæˆ:")
        print("python generate_with_custom_model.py")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥")

if __name__ == "__main__":
    main()