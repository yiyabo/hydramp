#!/usr/bin/env python3
"""
ä½¿ç”¨Gram-.fastaæ•°æ®è¿›è¡ŒHydrAMPæ¨¡å‹å¾®è°ƒè®­ç»ƒçš„å®ç”¨è„šæœ¬
è¿™ä¸ªæ–¹æ¡ˆåŸºäºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé¿å…ä»é›¶è®­ç»ƒçš„å›°éš¾
"""

import os
import sys
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

# è®¾ç½®GPUé…ç½®
config = tf.compat.v1.ConfigProto(
    gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8),
)
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(session)

# å¯¼å…¥HydrAMPæ¨¡å—
import amp.data_utils.sequence as du_sequence
from amp.config import MIN_LENGTH, MAX_LENGTH, LATENT_DIM, MIN_KL, RCL_WEIGHT, HIDDEN_DIM, MAX_TEMPERATURE
from amp.utils import basic_model_serializer
from keras import backend
from keras.optimizers import Adam

def prepare_gram_data(fasta_file):
    """
    å‡†å¤‡Gram-.fastaæ•°æ®ç”¨äºè®­ç»ƒ
    """
    print(f"æ­£åœ¨å¤„ç†FASTAæ–‡ä»¶: {fasta_file}")
    
    # å®šä¹‰æ ‡å‡†æ°¨åŸºé…¸
    STANDARD_AA = set('ACDEFGHIKLMNPQRSTVWY')
    
    def check_if_std_aa(seq):
        return all(aa in STANDARD_AA for aa in seq.upper())
    
    # è§£æFASTAæ–‡ä»¶
    sequences = []
    ids = []
    current_id = ''
    current_seq = ''
    
    with open(fasta_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('>'):
                if current_seq:
                    sequences.append(current_seq)
                    ids.append(current_id)
                current_id = line[1:]
                current_seq = ''
            else:
                current_seq += line
        
        # æ·»åŠ æœ€åä¸€ä¸ªåºåˆ—
        if current_seq:
            sequences.append(current_seq)
            ids.append(current_id)
    
    # è¿‡æ»¤åºåˆ—ï¼šé•¿åº¦åœ¨èŒƒå›´å†…ä¸”åªåŒ…å«æ ‡å‡†æ°¨åŸºé…¸
    filtered_sequences = []
    for seq in sequences:
        if MIN_LENGTH <= len(seq) <= MAX_LENGTH and check_if_std_aa(seq):
            filtered_sequences.append(seq)
    
    print(f"åŸå§‹åºåˆ—æ•°: {len(sequences)}")
    print(f"æœ‰æ•ˆåºåˆ—æ•°: {len(filtered_sequences)}")
    
    # è½¬æ¢ä¸ºone-hotç¼–ç å¹¶padding
    one_hot_sequences = du_sequence.pad(du_sequence.to_one_hot(filtered_sequences))
    labels = np.ones(len(filtered_sequences))  # å…¨éƒ¨æ ‡è®°ä¸ºæ­£æ ·æœ¬ï¼ˆAMPï¼‰
    
    return one_hot_sequences, labels, filtered_sequences

def fine_tune_with_gram_data(gram_sequences, gram_labels, original_sequences):
    """
    ä½¿ç”¨Gramæ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ
    """
    print("å¼€å§‹å¾®è°ƒè®­ç»ƒ...")
    
    # 1. åŠ è½½é¢„è®­ç»ƒçš„åˆ†ç±»å™¨
    print("åŠ è½½é¢„è®­ç»ƒåˆ†ç±»å™¨...")
    bms = basic_model_serializer.BasicModelSerializer()
    
    try:
        amp_classifier = bms.load_model('models/amp_classifier')
        mic_classifier = bms.load_model('models/mic_classifier')
        print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒåˆ†ç±»å™¨")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½é¢„è®­ç»ƒåˆ†ç±»å™¨: {e}")
        return False
    
    # 2. ä½¿ç”¨é¢„è®­ç»ƒåˆ†ç±»å™¨é¢„æµ‹Gramæ•°æ®çš„æ ‡ç­¾
    print("é¢„æµ‹Gramæ•°æ®çš„AMPå’ŒMICæ¦‚ç‡...")
    amp_classifier_model = amp_classifier()
    mic_classifier_model = mic_classifier()
    
    gram_amp_probs = amp_classifier_model.predict(gram_sequences, verbose=1).flatten()
    gram_mic_probs = mic_classifier_model.predict(gram_sequences, verbose=1).flatten()
    
    print(f"Gramæ•°æ®AMPæ¦‚ç‡ - å¹³å‡: {gram_amp_probs.mean():.4f}, èŒƒå›´: [{gram_amp_probs.min():.4f}, {gram_amp_probs.max():.4f}]")
    print(f"Gramæ•°æ®MICæ¦‚ç‡ - å¹³å‡: {gram_mic_probs.mean():.4f}, èŒƒå›´: [{gram_mic_probs.min():.4f}, {gram_mic_probs.max():.4f}]")
    
    # 3. è¿‡æ»¤é«˜è´¨é‡åºåˆ—ç”¨äºå¾®è°ƒ
    # é€‰æ‹©AMPæ¦‚ç‡é«˜çš„åºåˆ—è¿›è¡Œå¾®è°ƒ
    high_quality_mask = gram_amp_probs > 0.7  # åªä½¿ç”¨é«˜è´¨é‡é¢„æµ‹çš„åºåˆ—
    
    if high_quality_mask.sum() < 10:
        print("âš ï¸  é«˜è´¨é‡åºåˆ—å¤ªå°‘ï¼Œé™ä½é˜ˆå€¼...")
        high_quality_mask = gram_amp_probs > 0.5
    
    if high_quality_mask.sum() < 5:
        print("âš ï¸  é«˜è´¨é‡åºåˆ—ä»ç„¶å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰åºåˆ—...")
        high_quality_mask = np.ones(len(gram_sequences), dtype=bool)
    
    filtered_sequences = gram_sequences[high_quality_mask]
    filtered_amp_probs = gram_amp_probs[high_quality_mask]
    filtered_mic_probs = gram_mic_probs[high_quality_mask]
    filtered_original_seqs = [original_sequences[i] for i in range(len(original_sequences)) if high_quality_mask[i]]
    
    print(f"ç”¨äºå¾®è°ƒçš„é«˜è´¨é‡åºåˆ—æ•°: {len(filtered_sequences)}")
    
    # 4. æ•°æ®å¢å¼ºï¼šä¸ºå°‘é‡æ•°æ®åˆ›å»ºå˜ä½“
    print("è¿›è¡Œæ•°æ®å¢å¼º...")
    augmented_sequences = []
    augmented_amp_probs = []
    augmented_mic_probs = []
    
    for seq, amp_prob, mic_prob in zip(filtered_sequences, filtered_amp_probs, filtered_mic_probs):
        # åŸå§‹åºåˆ—
        augmented_sequences.append(seq)
        augmented_amp_probs.append(amp_prob)
        augmented_mic_probs.append(mic_prob)
        
        # æ·»åŠ è½»å¾®å™ªå£°çš„å˜ä½“ï¼ˆåªå¯¹é«˜è´¨é‡åºåˆ—ï¼‰
        if amp_prob > 0.8:
            for _ in range(3):  # æ¯ä¸ªé«˜è´¨é‡åºåˆ—åˆ›å»º3ä¸ªå˜ä½“
                noisy_seq = add_sequence_noise(seq)
                if noisy_seq is not None:
                    augmented_sequences.append(noisy_seq)
                    augmented_amp_probs.append(amp_prob * 0.95)  # ç•¥å¾®é™ä½æ¦‚ç‡
                    augmented_mic_probs.append(mic_prob * 0.95)
    
    augmented_sequences = np.array(augmented_sequences)
    augmented_amp_probs = np.array(augmented_amp_probs)
    augmented_mic_probs = np.array(augmented_mic_probs)
    
    print(f"æ•°æ®å¢å¼ºååºåˆ—æ•°: {len(augmented_sequences)}")
    
    # 5. ç®€åŒ–çš„å¾®è°ƒï¼šåªè®­ç»ƒåˆ†ç±»å™¨å¤´
    print("å¼€å§‹å¾®è°ƒåˆ†ç±»å™¨...")
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    train_seqs, val_seqs, train_amp, val_amp, train_mic, val_mic = train_test_split(
        augmented_sequences, augmented_amp_probs, augmented_mic_probs, 
        test_size=0.2, random_state=42
    )
    
    # å¾®è°ƒAMPåˆ†ç±»å™¨
    print("å¾®è°ƒAMPåˆ†ç±»å™¨...")
    amp_model = amp_classifier_model
    
    # å†»ç»“é™¤æœ€åå‡ å±‚å¤–çš„æ‰€æœ‰å±‚
    for layer in amp_model.layers[:-2]:
        layer.trainable = False
    
    # é‡æ–°ç¼–è¯‘æ¨¡å‹
    amp_model.compile(
        optimizer=Adam(lr=1e-4),  # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    # è®­ç»ƒ
    history_amp = amp_model.fit(
        train_seqs, train_amp,
        validation_data=(val_seqs, val_amp),
        epochs=10,
        batch_size=32,
        verbose=1
    )
    
    # å¾®è°ƒMICåˆ†ç±»å™¨
    print("å¾®è°ƒMICåˆ†ç±»å™¨...")
    mic_model = mic_classifier_model
    
    # å†»ç»“é™¤æœ€åå‡ å±‚å¤–çš„æ‰€æœ‰å±‚
    for layer in mic_model.layers[:-2]:
        layer.trainable = False
    
    mic_model.compile(
        optimizer=Adam(lr=1e-4),
        loss='mean_squared_error',
        metrics=['mae']
    )
    
    history_mic = mic_model.fit(
        train_seqs, train_mic,
        validation_data=(val_seqs, val_mic),
        epochs=10,
        batch_size=32,
        verbose=1
    )
    
    # 6. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
    print("ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹...")
    os.makedirs('models/gram_finetuned', exist_ok=True)
    
    amp_model.save('models/gram_finetuned/amp_classifier_finetuned.h5')
    mic_model.save('models/gram_finetuned/mic_classifier_finetuned.h5')
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    training_info = {
        'original_sequences_count': len(original_sequences),
        'filtered_sequences_count': len(filtered_sequences),
        'augmented_sequences_count': len(augmented_sequences),
        'training_sequences_count': len(train_seqs),
        'validation_sequences_count': len(val_seqs),
        'amp_final_loss': history_amp.history['loss'][-1],
        'amp_final_val_loss': history_amp.history['val_loss'][-1],
        'mic_final_loss': history_mic.history['loss'][-1],
        'mic_final_val_loss': history_mic.history['val_loss'][-1],
        'high_quality_sequences': filtered_original_seqs
    }
    
    import json
    with open('models/gram_finetuned/training_info.json', 'w') as f:
        json.dump(training_info, f, indent=2, default=str)
    
    print("âœ… å¾®è°ƒå®Œæˆï¼")
    print(f"æ¨¡å‹ä¿å­˜ä½ç½®: models/gram_finetuned/")
    print(f"è®­ç»ƒä¿¡æ¯ä¿å­˜ä½ç½®: models/gram_finetuned/training_info.json")
    
    return True

def add_sequence_noise(sequence, noise_prob=0.1):
    """
    ä¸ºåºåˆ—æ·»åŠ è½»å¾®å™ªå£°ï¼ˆæ°¨åŸºé…¸æ›¿æ¢ï¼‰
    """
    if len(sequence) == 0:
        return None
    
    # å°†sequenceè½¬æ¢ä¸ºå¯ä¿®æ”¹çš„åˆ—è¡¨
    seq_one_hot = sequence.copy()
    seq_length = np.sum(seq_one_hot.sum(axis=1) > 0)  # å®é™…åºåˆ—é•¿åº¦
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªä½ç½®è¿›è¡Œè½»å¾®ä¿®æ”¹
    if seq_length > 1 and np.random.random() < noise_prob:
        pos = np.random.randint(0, seq_length)
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ–°çš„æ°¨åŸºé…¸
        new_aa = np.random.randint(0, 20)
        seq_one_hot[pos] = 0  # æ¸…é›¶
        seq_one_hot[pos, new_aa] = 1  # è®¾ç½®æ–°æ°¨åŸºé…¸
    
    return seq_one_hot

def main():
    """
    ä¸»å‡½æ•°ï¼šä½¿ç”¨Gram-.fastaæ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒ
    """
    print("=== ä½¿ç”¨Gram-.fastaæ•°æ®å¾®è°ƒHydrAMPæ¨¡å‹ ===")
    
    # 1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    fasta_file = "Gram-.fasta"
    if not os.path.exists(fasta_file):
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {fasta_file}")
        return
    
    # 2. æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹
    required_models = ['models/amp_classifier', 'models/mic_classifier']
    for model_path in required_models:
        if not os.path.exists(model_path):
            print(f"âŒ æ‰¾ä¸åˆ°é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
            print("è¯·å…ˆè¿è¡Œ: sh get_data.sh")
            return
    
    # 3. å‡†å¤‡æ•°æ®
    print("æ­¥éª¤1: å‡†å¤‡Gramæ•°æ®...")
    gram_sequences, gram_labels, original_sequences = prepare_gram_data(fasta_file)
    
    if len(gram_sequences) < 10:
        print("âŒ æœ‰æ•ˆåºåˆ—å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
        return
    
    # 4. è¿›è¡Œå¾®è°ƒè®­ç»ƒ
    print("æ­¥éª¤2: å¼€å§‹å¾®è°ƒè®­ç»ƒ...")
    success = fine_tune_with_gram_data(gram_sequences, gram_labels, original_sequences)
    
    if success:
        print("\nğŸ‰ å¾®è°ƒè®­ç»ƒå®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("1. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆæ•°æ®:")
        print("   python generate_with_finetuned_model.py")
        print("2. æŸ¥çœ‹è®­ç»ƒä¿¡æ¯:")
        print("   cat models/gram_finetuned/training_info.json")
    else:
        print("\nâŒ å¾®è°ƒè®­ç»ƒå¤±è´¥")

if __name__ == "__main__":
    main()