#!/usr/bin/env python3
"""
ä½¿ç”¨Gramæ•°æ®å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆ300æ¡æŠ—èŒè‚½
"""

import os
import sys
import pandas as pd
import numpy as np
import json
from keras.models import load_model

# å¯¼å…¥HydrAMPæ¨¡å—
from amp.config import LATENT_DIM
from amp.data_utils import sequence as du_sequence
from amp.utils.basic_model_serializer import load_master_model_components
from amp.utils.generate_peptides import translate_peptide
from amp.utils.phys_chem_propterties import calculate_physchem_prop
from amp.utils.seed import set_seed
import joblib

def generate_with_finetuned_classifiers(n_target=300, seed=42):
    """
    ä½¿ç”¨å¾®è°ƒåçš„åˆ†ç±»å™¨ç”ŸæˆæŠ—èŒè‚½
    """
    print("=== ä½¿ç”¨Gramå¾®è°ƒæ¨¡å‹ç”ŸæˆæŠ—èŒè‚½ ===")
    
    # 1. æ£€æŸ¥å¾®è°ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨
    finetuned_amp_path = 'models/gram_finetuned/amp_classifier_finetuned.h5'
    finetuned_mic_path = 'models/gram_finetuned/mic_classifier_finetuned.h5'
    
    if not os.path.exists(finetuned_amp_path):
        print(f"âŒ æ‰¾ä¸åˆ°å¾®è°ƒçš„AMPåˆ†ç±»å™¨: {finetuned_amp_path}")
        print("è¯·å…ˆè¿è¡Œ: python train_with_gram_data.py")
        return None
    
    if not os.path.exists(finetuned_mic_path):
        print(f"âŒ æ‰¾ä¸åˆ°å¾®è°ƒçš„MICåˆ†ç±»å™¨: {finetuned_mic_path}")
        print("è¯·å…ˆè¿è¡Œ: python train_with_gram_data.py")
        return None
    
    # 2. åŠ è½½æ¨¡å‹ç»„ä»¶
    print("åŠ è½½æ¨¡å‹ç»„ä»¶...")
    
    # åŠ è½½åŸå§‹çš„ç¼–ç å™¨å’Œè§£ç å™¨
    model_path = "models/HydrAMP/37"
    decomposer_path = "models/HydrAMP/pca_decomposer.joblib"
    
    components = load_master_model_components(model_path, return_master=True, softmax=False)
    encoder, decoder, _, _, master = components
    
    # åŠ è½½å¾®è°ƒåçš„åˆ†ç±»å™¨
    print("åŠ è½½å¾®è°ƒåçš„åˆ†ç±»å™¨...")
    finetuned_amp_classifier = load_model(finetuned_amp_path)
    finetuned_mic_classifier = load_model(finetuned_mic_path)
    
    # åŠ è½½PCAåˆ†è§£å™¨
    latent_decomposer = joblib.load(decomposer_path)
    
    # 3. è¯»å–è®­ç»ƒä¿¡æ¯
    training_info_path = 'models/gram_finetuned/training_info.json'
    if os.path.exists(training_info_path):
        with open(training_info_path, 'r') as f:
            training_info = json.load(f)
        print(f"è®­ç»ƒä¿¡æ¯: ä½¿ç”¨äº†{training_info['original_sequences_count']}æ¡åŸå§‹Gramåºåˆ—")
        print(f"é«˜è´¨é‡åºåˆ—: {training_info['filtered_sequences_count']}æ¡")
        print(f"æ•°æ®å¢å¼ºå: {training_info['augmented_sequences_count']}æ¡")
    
    # 4. ç”Ÿæˆæ–°åºåˆ—
    print(f"å¼€å§‹ç”Ÿæˆ{n_target}æ¡åºåˆ—...")
    set_seed(seed)
    
    accepted_sequences = []
    accepted_amp = []
    accepted_mic = []
    
    batch_size = 100
    attempts_per_z = 64
    max_iterations = 20  # é˜²æ­¢æ— é™å¾ªç¯
    iteration = 0
    
    while len(accepted_sequences) < n_target and iteration < max_iterations:
        iteration += 1
        print(f"è¿­ä»£ {iteration}: å½“å‰å·²ç”Ÿæˆ {len(accepted_sequences)}/{n_target} æ¡åºåˆ—")
        
        # ç”Ÿæˆæ½œåœ¨å‘é‡
        current_batch = min(batch_size, n_target - len(accepted_sequences))
        z = np.random.normal(size=(current_batch, LATENT_DIM))
        z = latent_decomposer.inverse_transform(z)
        z = np.vstack([z] * attempts_per_z)
        
        # æ·»åŠ æ¡ä»¶ï¼ˆAMP=1, MIC=1ï¼‰
        c_amp = np.ones((z.shape[0], 1))
        c_mic = np.ones((z.shape[0], 1))
        z_cond = np.hstack([z, c_amp, c_mic])
        
        # è§£ç ç”Ÿæˆåºåˆ—
        candidate = decoder.predict(z_cond, verbose=0, batch_size=1000)
        candidate_index_decoded = candidate.argmax(axis=2)
        generated_sequences = [translate_peptide(pep) for pep in candidate_index_decoded]
        
        # ä½¿ç”¨å¾®è°ƒåçš„åˆ†ç±»å™¨é¢„æµ‹
        generated_amp = finetuned_amp_classifier.predict(candidate_index_decoded, verbose=0).flatten()
        generated_mic = finetuned_mic_classifier.predict(candidate_index_decoded, verbose=0).flatten()
        
        # é€‰æ‹©æœ€ä½³åºåˆ—
        generated_sequences = np.array(generated_sequences).reshape(attempts_per_z, -1)
        generated_amp = generated_amp.reshape(attempts_per_z, -1)
        generated_mic = generated_mic.reshape(attempts_per_z, -1)
        
        # å¯¹æ¯ä¸ªåŸå§‹zå‘é‡é€‰æ‹©æœ€ä½³åºåˆ—
        best_indices = generated_amp.argmax(axis=0)
        num_z_vectors = generated_sequences.shape[1]
        
        for i in range(num_z_vectors):
            best_idx = best_indices[i]
            seq = generated_sequences[best_idx, i]
            amp_prob = generated_amp[best_idx, i]
            mic_prob = generated_mic[best_idx, i]
            
            # è¿‡æ»¤æ¡ä»¶ï¼ˆç›¸æ¯”åŸç‰ˆæ›´ä¸¥æ ¼ï¼Œå› ä¸ºä½¿ç”¨äº†å¾®è°ƒæ¨¡å‹ï¼‰
            if (amp_prob > 0.85 and  # æé«˜AMPé˜ˆå€¼
                mic_prob > 0.3 and   # MICé˜ˆå€¼
                5 <= len(seq) <= 25 and 
                seq not in accepted_sequences and
                not has_problematic_patterns(seq)):
                
                accepted_sequences.append(seq)
                accepted_amp.append(amp_prob)
                accepted_mic.append(mic_prob)
                
                if len(accepted_sequences) >= n_target:
                    break
    
    if len(accepted_sequences) < n_target:
        print(f"âš ï¸  åªç”Ÿæˆäº†{len(accepted_sequences)}æ¡åºåˆ—ï¼ˆç›®æ ‡{n_target}æ¡ï¼‰")
        print("è¿™å¯èƒ½æ˜¯å› ä¸ºå¾®è°ƒæ¨¡å‹çš„è¿‡æ»¤æ¡ä»¶è¾ƒä¸¥æ ¼")
    
    # 5. è®¡ç®—ç‰©ç†åŒ–å­¦æ€§è´¨å¹¶ä¿å­˜
    if len(accepted_sequences) > 0:
        print("è®¡ç®—ç‰©ç†åŒ–å­¦æ€§è´¨...")
        physchem_props = calculate_physchem_prop(accepted_sequences)
        
        # åˆ›å»ºç»“æœDataFrame
        result_data = {
            'sequence': accepted_sequences,
            'amp': accepted_amp,
            'mic': accepted_mic,
            'length': [len(seq) for seq in accepted_sequences],
            'model_type': ['gram_finetuned'] * len(accepted_sequences)  # æ ‡è®°æ¨¡å‹ç±»å‹
        }
        
        # æ·»åŠ ç‰©ç†åŒ–å­¦æ€§è´¨
        result_data.update(physchem_props)
        
        df = pd.DataFrame(result_data)
        
        # ä¿å­˜ç»“æœ
        output_file = f"gram_finetuned_generated_{len(df)}.csv"
        df.to_csv(output_file, index=False)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n=== ç”Ÿæˆæ•°æ®ç»Ÿè®¡ ===")
        print(f"æ€»åºåˆ—æ•°: {len(df)}")
        print(f"å¹³å‡é•¿åº¦: {df['length'].mean():.2f}")
        print(f"é•¿åº¦èŒƒå›´: {df['length'].min()}-{df['length'].max()}")
        print(f"å¹³å‡AMPæ¦‚ç‡: {df['amp'].mean():.4f}")
        print(f"å¹³å‡MICæ¦‚ç‡: {df['mic'].mean():.4f}")
        
        # æ¯”è¾ƒåŸå§‹Gramæ•°æ®
        if os.path.exists(training_info_path):
            print(f"\n=== ä¸åŸå§‹Gramæ•°æ®å¯¹æ¯” ===")
            high_quality_seqs = training_info.get('high_quality_sequences', [])
            if high_quality_seqs:
                print(f"åŸå§‹Gramé«˜è´¨é‡åºåˆ—æ•°: {len(high_quality_seqs)}")
                print(f"ç”Ÿæˆåºåˆ—æ•°: {len(df)}")
                print(f"æ‰©å±•å€æ•°: {len(df)/len(high_quality_seqs):.1f}x")
        
        print("\nå‰5ä¸ªç”Ÿæˆçš„åºåˆ—:")
        for i in range(min(5, len(df))):
            row = df.iloc[i]
            print(f"{i+1}. {row['sequence']} (é•¿åº¦:{row['length']}, AMP:{row['amp']:.4f})")
        
        return df
    else:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç¬¦åˆæ¡ä»¶çš„åºåˆ—")
        return None

def has_problematic_patterns(seq):
    """æ£€æŸ¥åºåˆ—æ˜¯å¦æœ‰é—®é¢˜æ¨¡å¼"""
    # æ£€æŸ¥è¿ç»­çš„ç–æ°´æ°¨åŸºé…¸
    hydrophobic = 'AILMFPWVY'
    consecutive_hydrophobic = 0
    max_consecutive_hydrophobic = 0
    
    for i, aa in enumerate(seq):
        if aa in hydrophobic:
            consecutive_hydrophobic += 1
            max_consecutive_hydrophobic = max(max_consecutive_hydrophobic, consecutive_hydrophobic)
        else:
            consecutive_hydrophobic = 0
        
        # æ£€æŸ¥5ä¸ªæ°¨åŸºé…¸çª—å£å†…çš„é‡å¤
        if i >= 4:
            window = seq[i-4:i+1]
            unique_count = len(set(window))
            if unique_count <= 2:
                return True
    
    # è¿ç»­ç–æ°´æ°¨åŸºé…¸è¿‡å¤š
    if max_consecutive_hydrophobic > 3:
        return True
    
    # æ£€æŸ¥åŠèƒ±æ°¨é…¸
    if 'C' in seq:
        return True
    
    return False

def main():
    """ä¸»å‡½æ•°"""
    print("=== ä½¿ç”¨Gramå¾®è°ƒæ¨¡å‹ç”ŸæˆæŠ—èŒè‚½ ===")
    
    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆå¾®è°ƒè®­ç»ƒ
    if not os.path.exists('models/gram_finetuned/amp_classifier_finetuned.h5'):
        print("âŒ æœªæ‰¾åˆ°å¾®è°ƒåçš„æ¨¡å‹")
        print("è¯·å…ˆè¿è¡Œ: python train_with_gram_data.py")
        return
    
    # ç”Ÿæˆæ•°æ®
    df = generate_with_finetuned_classifiers(n_target=300, seed=42)
    
    if df is not None:
        print(f"\nğŸ‰ æˆåŠŸä½¿ç”¨Gramå¾®è°ƒæ¨¡å‹ç”Ÿæˆ{len(df)}æ¡æŠ—èŒè‚½ï¼")
        print("è¿™äº›åºåˆ—æ˜¯åŸºäºä½ çš„Gram-.fastaæ•°æ®è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆçš„ã€‚")
    else:
        print("\nâŒ ç”Ÿæˆå¤±è´¥")

if __name__ == "__main__":
    main()